import os
import time
import json
import hashlib
import requests
from datetime import datetime
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import feedparser  # RSS íŒŒì‹±ìš©

# .env íŒŒì¼ ë¡œë“œ
env_path = Path(__file__).parent / '.env'
print(f"ğŸ“‚ .env íŒŒì¼ ê²½ë¡œ: {env_path}")
print(f"ğŸ“‚ .env íŒŒì¼ ì¡´ì¬: {env_path.exists()}")
load_dotenv(dotenv_path=env_path)

# API í‚¤ ë° ì„¤ì •
GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY', '')
GROK_API_KEY = os.environ.get('GROK_API_KEY', '')
AI_PROVIDER = os.environ.get('AI_PROVIDER', 'gemini').lower()  # 'gemini' ë˜ëŠ” 'grok'
BACKEND_URL = 'http://localhost:3001'

# ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
DATA_DIR = Path(__file__).parent.parent / 'data'
DATA_DIR.mkdir(exist_ok=True)
NEWS_FILE = DATA_DIR / 'news.json'
RAW_NEWS_FILE = DATA_DIR / 'raw-news.json'  # ì˜ì–´ ì›ë³¸ (ì¤‘ë³µ ì²´í¬ìš©)

print(f"ğŸ¤– AI Provider: {AI_PROVIDER.upper()}")
print(f"ğŸ”— Backend URL: {BACKEND_URL}")
print(f"ğŸ’¾ Data directory: {DATA_DIR}")
print(f"ğŸ“„ Raw news file: {RAW_NEWS_FILE}")
print(f"ğŸ”‘ GEMINI_API_KEY: {'âœ“ ' + GEMINI_API_KEY[:20] + '...' if GEMINI_API_KEY else 'âœ— ì—†ìŒ'}")
print(f"ğŸ”‘ GROK_API_KEY: {'âœ“ ' + GROK_API_KEY[:20] + '...' if GROK_API_KEY else 'âœ— ì—†ìŒ'}")

if AI_PROVIDER == 'gemini':
    if GEMINI_API_KEY:
        genai.configure(api_key=GEMINI_API_KEY)
        print(f"âœ“ Gemini configured successfully")
    else:
        print(f"âš ï¸  Warning: GEMINI_API_KEY not set")
elif AI_PROVIDER == 'grok':
    if GROK_API_KEY:
        print(f"âœ“ Grok configured successfully")
    else:
        print(f"âš ï¸  Warning: GROK_API_KEY not set")

class GlobeNewswireScraper:
    def __init__(self):
        self.seen_news = set()
        self.driver = None
        self.raw_news_file = RAW_NEWS_FILE
        self.load_raw_ids()  # raw-news.jsonì—ì„œ ì´ë¯¸ ìŠ¤í¬ë©ëœ ID ë¡œë“œ
        
    def load_raw_ids(self):
        """raw-news.jsonì—ì„œ ì´ë¯¸ ìŠ¤í¬ë©ëœ IDë§Œ ë¡œë“œ"""
        try:
            if self.raw_news_file.exists():
                with open(self.raw_news_file, 'r', encoding='utf-8') as f:
                    raw_news = json.load(f)
                    self.seen_news = set(item['id'] for item in raw_news)
                    print(f"ğŸ“‚ Raw news ë¡œë“œ: {len(self.seen_news)}ê°œ ID")
            else:
                print(f"ğŸ“‚ Raw news íŒŒì¼ ì—†ìŒ (ì‹ ê·œ ì‹œì‘)")
        except Exception as e:
            print(f"âš ï¸  Raw news ë¡œë“œ ì˜¤ë¥˜: {e}")
            self.seen_news = set()
    
    def save_raw_news(self, news_data):
        """ì˜ì–´ ì›ë³¸ì„ raw-news.jsonì— ì €ì¥ (API í˜¸ì¶œ ì „)"""
        try:
            # 1. ê¸°ì¡´ raw news ë¡œë“œ
            raw_news = []
            if self.raw_news_file.exists():
                with open(self.raw_news_file, 'r', encoding='utf-8') as f:
                    raw_news = json.load(f)
            
            # 2. ìƒˆ ë‰´ìŠ¤ ì¶”ê°€
            raw_news.insert(0, news_data)
            
            # 3. ìµœëŒ€ 5000ê°œ ìœ ì§€ (ìš©ëŸ‰ ê´€ë¦¬)
            raw_news = raw_news[:5000]
            
            # 4. ì €ì¥
            with open(self.raw_news_file, 'w', encoding='utf-8') as f:
                json.dump(raw_news, f, indent=2, ensure_ascii=False)
            
            # 5. seen_news ì—…ë°ì´íŠ¸
            self.seen_news.add(news_data['id'])
            
        except Exception as e:
            print(f"âœ— Raw news ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def setup_driver(self):
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        
        self.driver = webdriver.Chrome(options=chrome_options)
        
    def parse_news_item(self, element):
        """ë‰´ìŠ¤ í•­ëª© íŒŒì‹±"""
        try:
            # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ - .mainLink aë¥¼ ìš°ì„  ì°¾ê¸°
            link_elem = None
            link = None
            title = None
            
            # 1ìˆœìœ„: .mainLink a (ë³¸ë¬¸ ë§í¬)
            try:
                link_elem = element.find_element(By.CSS_SELECTOR, '.mainLink a')
                link = link_elem.get_attribute('href')
                title = link_elem.text.strip()
            except:
                # 2ìˆœìœ„: ì¼ë°˜ a íƒœê·¸
                try:
                    link_elem = element.find_element(By.CSS_SELECTOR, 'a')
                    link = link_elem.get_attribute('href')
                    title = link_elem.text.strip()
                except:
                    return None
            
            if not link or not title:
                return None
            
            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if link.startswith('/'):
                link = f"https://www.globenewswire.com{link}"
            elif not link.startswith('http'):
                link = f"https://www.globenewswire.com/{link}"
            
            # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            full_text = element.text.strip()
            lines = [line.strip() for line in full_text.split('\n') if line.strip()]
            
            # ì²« ì¤„ì€ ë³´í†µ ì‹œê°„/ì†ŒìŠ¤ ì •ë³´
            time_line = lines[0] if lines else ''
            
            # ì‹œê°„ íŒŒì‹± (ET ê¸°ì¤€)
            published_time = ''
            source = ''
            
            # "October 17, 2025 05:05 ET | Source: Company Name" í˜•ì‹ íŒŒì‹±
            if 'ET' in time_line:
                # "October 17, 2025 05:05 ET" ë¶€ë¶„ë§Œ ì¶”ì¶œ
                if '|' in time_line:
                    parts = time_line.split('|')
                    published_time = parts[0].strip()
                    
                    # Source ì¶”ì¶œ
                    if 'Source:' in time_line:
                        source_part = [p for p in parts if 'Source:' in p]
                        if source_part:
                            source = source_part[0].replace('Source:', '').strip()
                else:
                    published_time = time_line.strip()
            elif time_line:
                # ETê°€ ì—†ì–´ë„ ë‚ ì§œ í˜•ì‹ì´ë©´ ì‚¬ìš©
                published_time = time_line.strip()
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ
            image_url = None
            try:
                img_elem = element.find_element(By.CSS_SELECTOR, 'img')
                image_url = img_elem.get_attribute('src')
            except:
                pass
            
            # ê³ ìœ  ID ìƒì„±
            news_id = hashlib.md5(f"{link}_{title}".encode()).hexdigest()
            
            return {
                'id': news_id,
                'region': 'Y',
                'publishedTime': published_time,
                'localTime': published_time,
                'usTime': published_time,
                'koTime': self.convert_to_korea_time(published_time),
                'source': source,
                'title': title,
                'link': link,
                'imageUrl': image_url
            }
        except Exception as e:
            print(f"Error parsing news item: {e}")
            return None
    
    def convert_to_korea_time(self, et_time):
        """ET ì‹œê°„ì„ í•œêµ­ ì‹œê°„ìœ¼ë¡œ ë³€í™˜"""
        try:
            from datetime import datetime, timedelta
            
            # ê°„ë‹¨í•œ ë³€í™˜ (ETëŠ” UTC-5, í•œêµ­ì€ UTC+9 = 14ì‹œê°„ ì°¨ì´)
            if 'ET' in et_time:
                return et_time.replace('ET', 'KST (+14h)')
            return et_time
        except:
            return et_time
    
    def fetch_news_content(self, url):
        """ë‰´ìŠ¤ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            response = requests.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'lxml')
            
            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (GlobeNewswire êµ¬ì¡°ì— ë§ê²Œ ì¡°ì •)
            content = soup.find('div', class_='article-body') or soup.find('div', class_='main-article')
            if content:
                return content.get_text(strip=True, separator=' ')[:2000]
            return ""
        except Exception as e:
            print(f"Error fetching content: {e}")
            return ""
    
    def validate_ticker_with_backend(self, ticker, text):
        """ë°±ì—”ë“œ APIë¡œ í‹°ì»¤ ê²€ì¦"""
        try:
            response = requests.post(
                f"{BACKEND_URL}/api/validate-ticker",
                json={'ticker': ticker, 'text': text},
                timeout=5
            )
            if response.status_code == 200:
                return response.json()
            return None
        except Exception as e:
            print(f"âš ï¸  Ticker validation failed: {e}")
            return None

    def analyze_with_ai(self, title, content, link):
        """AI APIë¡œ ë‰´ìŠ¤ ë¶„ì„ - íšŒì‚¬ëª… ìš°ì„  ì¶”ì¶œ í›„ ë°±ì—”ë“œì—ì„œ í‹°ì»¤ ê²€ì¦"""
        if AI_PROVIDER == 'grok':
            return self.analyze_with_grok(title, content, link)
        else:
            return self.analyze_with_gemini(title, content, link)
    
    def analyze_with_grok(self, title, content, link):
        """Grok APIë¡œ ë‰´ìŠ¤ ë¶„ì„"""
        if not GROK_API_KEY:
            print("Warning: GROK_API_KEY not set, skipping analysis")
            return None
        
        # Timeout ë°œìƒ ì‹œ ì¬ì‹œë„ (ìµœëŒ€ 2ë²ˆ)
        max_retries = 2
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    print(f"ğŸ”„ Retry {attempt}/{max_retries-1}...")
                    time.sleep(2)  # 2ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
                
                prompt = f"""Analyze this financial news and extract information IN KOREAN.

Title: {title}
Content: {content[:2000]}

**MOST IMPORTANT - Company Name First:**
1. Extract the EXACT company name from the news (e.g., "Apple Inc.", "Microsoft Corporation", "Tesla, Inc.")
   - Look for the company that is the MAIN SUBJECT of the news
   - NOT companies mentioned in passing
   - NOT author names, NOT analyst names
   - If this is advertising/promotional content (expert explains, tips, advice), return null

2. Ticker symbol (optional):
   - Only if you are 100% certain
   - MUST match the company name you extracted
   - If uncertain, leave as null

3. Sentiment: positive/negative/neutral

4. Calculate percentage split (must total 100%)
   - í˜¸ì¬ ìš”ì†Œì™€ ì•…ì¬ ìš”ì†Œë¥¼ **ê· í˜•ìˆê²Œ** í‰ê°€
   - ê³¼ë„í•˜ê²Œ ê¸ì •ì ì´ê±°ë‚˜ ë¶€ì •ì ì´ì§€ ì•Šê²Œ

5. Rise score (0-100): **BE CONSERVATIVE AND REALISTIC**
   **í‰ê°€ ê¸°ì¤€:**
   - 90-100ì : ë§¤ìš° í™•ì‹¤í•œ í˜¸ì¬ (ì‹¤ì  ëŒ€í­ ì¦ê°€, ëŒ€í˜• ê³„ì•½ ì²´ê²°, í˜ì‹ ì  ì‹ ì œí’ˆ ì¶œì‹œ)
   - 80-89ì : ê°•ë ¥í•œ í˜¸ì¬ (ì¤‘ìš”í•œ íŒŒíŠ¸ë„ˆì‹­, ì‹ ê·œ ì‚¬ì—… ì§„ì¶œ)
   - 70-79ì : ê¸ì •ì  (ì„ì› ì„ëª…, ì œí’ˆ ì¶œì‹œ, íˆ¬ì ìœ ì¹˜)
   - 60-69ì : ì•½ê°„ ê¸ì •ì  (ì¼ë°˜ ë°œí‘œ, ì´ë²¤íŠ¸ ì°¸ì—¬)
   - 50-59ì : ì¤‘ë¦½ (ì¬ë¬´ ê²°ê³¼ ë°œí‘œ, ì¼ë°˜ ê³µì‹œ, ë°°ë‹¹ ë°œí‘œ)
   - 40ì  ì´í•˜: ë¶€ì •ì  ì†Œì‹
   
   **ì£¼ì˜ì‚¬í•­:**
   - ë‹¨ìˆœ ê³µì‹œ/ë°œí‘œ: 50-60ì 
   - ì„ì› ì„ëª…/ì¸ì‚¬: 70ì  ì „í›„
   - ë°°ë‹¹/ë¶„ë°° ë°œí‘œ: 50-55ì 
   - ì¬ë¬´ ì‹¤ì  ë°œí‘œ(ë‚´ìš© ëª¨ë¦„): 50ì 
   - **ê³¼ì¥í•˜ì§€ ë§ê³  í˜„ì‹¤ì ìœ¼ë¡œ í‰ê°€**

6. **Translate title and description to Korean**

**CRITICAL - EXCLUDE these types of news:**
- Expert advice, tips, how-to guides
- HelloNation, Edvertising platforms
- General market reports (not specific company)
- Example: "Roofing expert explains..." â†’ companyName: null

**For advertising/non-company news:**
{{
    "companyName": null,
    "ticker": null,
    "sentiment": "neutral",
    "positivePercentage": 50,
    "negativePercentage": 50,
    "riseScore": 50,
    "summary": "ì¼ë°˜ ë‰´ìŠ¤",
    "titleKo": "ë²ˆì—­ëœ ì œëª©",
    "descriptionKo": "ë²ˆì—­ëœ ì„¤ëª…"
}}

**For real company news:**
{{
    "companyName": "Apple Inc.",
    "ticker": null,
    "sentiment": "positive",
    "positivePercentage": 85,
    "negativePercentage": 15,
    "riseScore": 75,
    "summary": "ì• í”Œì´ ì‹ ì œí’ˆ ì¶œì‹œ ë°œí‘œ",
    "titleKo": "ì• í”Œ, ìƒˆë¡œìš´ ì•„ì´í° 15 ë°œí‘œ",
    "descriptionKo": "ì• í”Œì´ ìµœì‹  ì•„ì´í° 15ë¥¼ ê³µê°œí•˜ë©° ë§¤ì¶œ ì¦ê°€ ì˜ˆìƒ"
}}

Remember: Company name is REQUIRED for stock news. Ticker is optional. MUST include titleKo and descriptionKo. Return ONLY valid JSON."""
                
                response = requests.post(
                    'https://api.x.ai/v1/chat/completions',
                    headers={
                        'Content-Type': 'application/json',
                        'Authorization': f'Bearer {GROK_API_KEY}'
                    },
                    json={
                        'messages': [
                            {'role': 'system', 'content': 'You are a financial news analyst. Always respond with valid JSON only, no markdown or extra text.'},
                            {'role': 'user', 'content': prompt}
                        ],
                        'model': 'grok-4-latest',
                        'stream': False,
                        'temperature': 0.3
                    },
                    timeout=30
                )
                
                if response.status_code != 200:
                    print(f"âœ— Grok API error: {response.status_code} {response.text}")
                    if attempt < max_retries - 1:
                        continue
                    return None
                
                result_text = response.json()['choices'][0]['message']['content'].strip()
                
                # JSON ì¶”ì¶œ
                if '```json' in result_text:
                    result_text = result_text.split('```json')[1].split('```')[0].strip()
                elif '```' in result_text:
                    result_text = result_text.split('```')[1].split('```')[0].strip()
                
                # { } ì‚¬ì´ ì¶”ì¶œ
                if '{' in result_text and '}' in result_text:
                    start = result_text.index('{')
                    end = result_text.rindex('}') + 1
                    result_text = result_text[start:end]
                
                analysis = json.loads(result_text)
                
                # íšŒì‚¬ëª…ì´ ì—†ìœ¼ë©´ ì¦‰ì‹œ ê±°ë¶€
                company_name = analysis.get('companyName')
                if not company_name or company_name.strip() == '':
                    print(f"âœ— No company name extracted (advertising/non-company news)")
                    analysis['isNasdaqListed'] = False
                    analysis['ticker'] = None
                    return analysis
                
                print(f"âœ“ Extracted company: {company_name}")
                
                # ë°±ì—”ë“œ APIë¡œ íšŒì‚¬ëª… ê¸°ë°˜ í‹°ì»¤ ê²€ìƒ‰
                try:
                    response = requests.post(
                        f"{BACKEND_URL}/api/validate-ticker",
                        json={'ticker': '', 'text': company_name},
                        timeout=5
                    )
                    if response.status_code == 200:
                        validation = response.json()
                        
                        if validation.get('isValid') and validation.get('stockInfo'):
                            stock_info = validation['stockInfo']
                            validated_ticker = stock_info.get('symbol')
                            validated_name = stock_info.get('name')
                            
                            print(f"âœ“ Backend matched: {validated_ticker} = {validated_name}")
                            
                            analysis['ticker'] = validated_ticker
                            analysis['isNasdaqListed'] = True
                            
                            # AIê°€ ì¶”ì¶œí•œ í‹°ì»¤ì™€ ë‹¤ë¥´ë©´ ê²½ê³ 
                            ai_ticker = analysis.get('ticker')
                            if ai_ticker and ai_ticker != validated_ticker:
                                print(f"âš ï¸  Ticker corrected: {ai_ticker} â†’ {validated_ticker}")
                        else:
                            print(f"âœ— Not found in NASDAQ: {company_name}")
                            analysis['isNasdaqListed'] = False
                            analysis['ticker'] = None
                    else:
                        print(f"âœ— Backend validation failed")
                        analysis['isNasdaqListed'] = False
                        analysis['ticker'] = None
                except Exception as e:
                    print(f"âœ— Backend API error: {e}")
                    analysis['isNasdaqListed'] = False
                    analysis['ticker'] = None
                
                print(f"âœ“ Final result: Listed={analysis.get('isNasdaqListed')}, Ticker={analysis.get('ticker')}, Company={company_name}")
                return analysis
                
            except (requests.exceptions.ReadTimeout, requests.exceptions.Timeout) as e:
                print(f"âœ— Grok timeout: {e}")
                if attempt < max_retries - 1:
                    continue
                return None
            except Exception as e:
                print(f"âœ— Grok analysis error: {e}")
                import traceback
                traceback.print_exc()
                if attempt < max_retries - 1:
                    continue
                return None
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        return None
    
    def analyze_batch_with_gemini(self, news_list):
        """Gemini APIë¡œ ì—¬ëŸ¬ ë‰´ìŠ¤ë¥¼ ë°°ì¹˜ ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬)"""
        if not GEMINI_API_KEY:
            print("Warning: GEMINI_API_KEY not set, skipping batch analysis")
            return []
        
        print(f"\nğŸš€ Batch analysis: {len(news_list)} ë‰´ìŠ¤ ë™ì‹œ ë¶„ì„ ì¤‘...")
        results = []
        
        # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬ (ìµœëŒ€ 5ê°œ ë™ì‹œ)
        with ThreadPoolExecutor(max_workers=5) as executor:
            future_to_news = {
                executor.submit(self.analyze_with_gemini, news['title'], news['content'], news['link']): news 
                for news in news_list
            }
            
            for future in as_completed(future_to_news):
                news = future_to_news[future]
                try:
                    result = future.result()
                    if result:
                        results.append({'news': news, 'analysis': result})
                except Exception as e:
                    print(f"âœ— Batch analysis error for {news['title'][:30]}...: {e}")
        
        print(f"âœ“ Batch analysis complete: {len(results)}/{len(news_list)} ì„±ê³µ")
        return results
    
    def analyze_with_gemini(self, title, content, link):
        """Gemini APIë¡œ ë‰´ìŠ¤ ë¶„ì„ - íšŒì‚¬ëª… ìš°ì„  ì¶”ì¶œ í›„ ë°±ì—”ë“œì—ì„œ í‹°ì»¤ ê²€ì¦"""
        if not GEMINI_API_KEY:
            return None
            
        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
            
            prompt = f"""Analyze this financial news and extract information IN KOREAN.

Title: {title}
Content: {content[:2000]}

**MOST IMPORTANT - Company Name First:**
1. Extract the EXACT company name from the news (e.g., "Apple Inc.", "Microsoft Corporation", "Tesla, Inc.")
   - Look for the company that is the MAIN SUBJECT of the news
   - NOT companies mentioned in passing
   - NOT author names, NOT analyst names
   - If this is advertising/promotional content (expert explains, tips, advice), return null

2. Ticker symbol (optional):
   - Only if you are 100% certain
   - MUST match the company name you extracted
   - If uncertain, leave as null

3. Sentiment: positive/negative/neutral

4. Calculate percentage split (must total 100%)
   - í˜¸ì¬ ìš”ì†Œì™€ ì•…ì¬ ìš”ì†Œë¥¼ **ê· í˜•ìˆê²Œ** í‰ê°€
   - ê³¼ë„í•˜ê²Œ ê¸ì •ì ì´ê±°ë‚˜ ë¶€ì •ì ì´ì§€ ì•Šê²Œ

5. Rise score (0-100): **BE CONSERVATIVE AND REALISTIC**
   **í‰ê°€ ê¸°ì¤€:**
   - 90-100ì : ë§¤ìš° í™•ì‹¤í•œ í˜¸ì¬ (ì‹¤ì  ëŒ€í­ ì¦ê°€, ëŒ€í˜• ê³„ì•½ ì²´ê²°, í˜ì‹ ì  ì‹ ì œí’ˆ ì¶œì‹œ)
   - 80-89ì : ê°•ë ¥í•œ í˜¸ì¬ (ì¤‘ìš”í•œ íŒŒíŠ¸ë„ˆì‹­, ì‹ ê·œ ì‚¬ì—… ì§„ì¶œ)
   - 70-79ì : ê¸ì •ì  (ì„ì› ì„ëª…, ì œí’ˆ ì¶œì‹œ, íˆ¬ì ìœ ì¹˜)
   - 60-69ì : ì•½ê°„ ê¸ì •ì  (ì¼ë°˜ ë°œí‘œ, ì´ë²¤íŠ¸ ì°¸ì—¬)
   - 50-59ì : ì¤‘ë¦½ (ì¬ë¬´ ê²°ê³¼ ë°œí‘œ, ì¼ë°˜ ê³µì‹œ, ë°°ë‹¹ ë°œí‘œ)
   - 40ì  ì´í•˜: ë¶€ì •ì  ì†Œì‹
   
   **ì£¼ì˜ì‚¬í•­:**
   - ë‹¨ìˆœ ê³µì‹œ/ë°œí‘œ: 50-60ì 
   - ì„ì› ì„ëª…/ì¸ì‚¬: 70ì  ì „í›„
   - ë°°ë‹¹/ë¶„ë°° ë°œí‘œ: 50-55ì 
   - ì¬ë¬´ ì‹¤ì  ë°œí‘œ(ë‚´ìš© ëª¨ë¦„): 50ì 
   - **ê³¼ì¥í•˜ì§€ ë§ê³  í˜„ì‹¤ì ìœ¼ë¡œ í‰ê°€**

6. **Translate title and description to Korean**

**CRITICAL - EXCLUDE these types of news:**
- Expert advice, tips, how-to guides
- HelloNation, Edvertising platforms
- General market reports (not specific company)
- Example: "Roofing expert explains..." â†’ companyName: null

**For advertising/non-company news:**
{{
    "companyName": null,
    "ticker": null,
    "sentiment": "neutral",
    "positivePercentage": 50,
    "negativePercentage": 50,
    "riseScore": 50,
    "summary": "ì¼ë°˜ ë‰´ìŠ¤",
    "titleKo": "ë²ˆì—­ëœ ì œëª©",
    "descriptionKo": "ë²ˆì—­ëœ ì„¤ëª…"
}}

**For real company news:**
{{
    "companyName": "Apple Inc.",
    "ticker": null,
    "sentiment": "positive",
    "positivePercentage": 85,
    "negativePercentage": 15,
    "riseScore": 75,
    "summary": "ì• í”Œì´ ì‹ ì œí’ˆ ì¶œì‹œ ë°œí‘œ",
    "titleKo": "ì• í”Œ, ìƒˆë¡œìš´ ì•„ì´í° 15 ë°œí‘œ",
    "descriptionKo": "ì• í”Œì´ ìµœì‹  ì•„ì´í° 15ë¥¼ ê³µê°œí•˜ë©° ë§¤ì¶œ ì¦ê°€ ì˜ˆìƒ"
}}

Remember: Company name is REQUIRED for stock news. Ticker is optional. MUST include titleKo and descriptionKo. Return ONLY valid JSON."""
            
            response = model.generate_content(prompt)
            result_text = response.text.strip()
            
            # JSON ì¶”ì¶œ
            if '```json' in result_text:
                result_text = result_text.split('```json')[1].split('```')[0].strip()
            elif '```' in result_text:
                result_text = result_text.split('```')[1].split('```')[0].strip()
            
            # { } ì‚¬ì´ ì¶”ì¶œ
            if '{' in result_text and '}' in result_text:
                start = result_text.index('{')
                end = result_text.rindex('}') + 1
                result_text = result_text[start:end]
            
            analysis = json.loads(result_text)
            
            # íšŒì‚¬ëª…ì´ ì—†ìœ¼ë©´ ì¦‰ì‹œ ê±°ë¶€
            company_name = analysis.get('companyName')
            if not company_name or company_name.strip() == '':
                print(f"âœ— No company name extracted (advertising/non-company news)")
                analysis['isNasdaqListed'] = False
                analysis['ticker'] = None
                return analysis
            
            print(f"âœ“ Extracted company: {company_name}")
            
            # ë°±ì—”ë“œ APIë¡œ íšŒì‚¬ëª… ê¸°ë°˜ í‹°ì»¤ ê²€ìƒ‰
            try:
                response = requests.post(
                    f"{BACKEND_URL}/api/validate-ticker",
                    json={'ticker': '', 'text': company_name},
                    timeout=5
                )
                if response.status_code == 200:
                    validation = response.json()
                    
                    if validation.get('isValid') and validation.get('stockInfo'):
                        stock_info = validation['stockInfo']
                        validated_ticker = stock_info.get('symbol')
                        validated_name = stock_info.get('name')
                        
                        print(f"âœ“ Backend matched: {validated_ticker} = {validated_name}")
                        
                        analysis['ticker'] = validated_ticker
                        analysis['isNasdaqListed'] = True
                        
                        # Geminiê°€ ì¶”ì¶œí•œ í‹°ì»¤ì™€ ë‹¤ë¥´ë©´ ê²½ê³ 
                        gemini_ticker = analysis.get('ticker')
                        if gemini_ticker and gemini_ticker != validated_ticker:
                            print(f"âš ï¸  Ticker corrected: {gemini_ticker} â†’ {validated_ticker}")
                    else:
                        print(f"âœ— Not found in NASDAQ: {company_name}")
                        analysis['isNasdaqListed'] = False
                        analysis['ticker'] = None
                else:
                    print(f"âœ— Backend validation failed")
                    analysis['isNasdaqListed'] = False
                    analysis['ticker'] = None
            except Exception as e:
                print(f"âœ— Backend API error: {e}")
                analysis['isNasdaqListed'] = False
                analysis['ticker'] = None
            
            print(f"âœ“ Final result: Listed={analysis.get('isNasdaqListed')}, Ticker={analysis.get('ticker')}, Company={company_name}")
            return analysis
            
        except Exception as e:
            print(f"âœ— Gemini analysis error: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def translate_with_ai(self, text, max_retries=2):
        """AI APIë¡œ ë²ˆì—­ - Grokì€ ë¹„ìš© ì ˆê°ì„ ìœ„í•´ Gemini ì‚¬ìš©"""
        # Grokì€ ë¹„ìš©ì´ ë¹„ì‹¸ë¯€ë¡œ ë²ˆì—­ì€ í•­ìƒ Gemini ì‚¬ìš©
        return self.translate_with_gemini(text, max_retries)
    
    def translate_with_grok(self, text, max_retries=2):
        """Grok APIë¡œ ë²ˆì—­"""
        if not GROK_API_KEY:
            print("Warning: GROK_API_KEY not set, skipping translation")
            return text
        
        if not text or len(text.strip()) == 0:
            return text
        
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    'https://api.x.ai/v1/chat/completions',
                    headers={
                        'Content-Type': 'application/json',
                        'Authorization': f'Bearer {GROK_API_KEY}'
                    },
                    json={
                        'messages': [
                            {'role': 'system', 'content': 'You are a professional translator. Translate English to Korean naturally.'},
                            {'role': 'user', 'content': f'Translate to Korean (output only Korean, no English):\n\n{text}'}
                        ],
                        'model': 'grok-2-latest',
                        'stream': False,
                        'temperature': 0.3
                    },
                    timeout=30
                )
                
                if response.status_code == 200:
                    translated = response.json()['choices'][0]['message']['content'].strip()
                    
                    # ë²ˆì—­ì´ ì œëŒ€ë¡œ ë˜ì—ˆëŠ”ì§€ í™•ì¸ (í•œê¸€ í¬í•¨ ì—¬ë¶€)
                    if any('\uac00' <= c <= '\ud7a3' for c in translated):
                        print(f"âœ“ Translated: {text[:40]}... -> {translated[:40]}...")
                        return translated
                    else:
                        print(f"âœ— Translation failed (no Korean detected), retry {attempt+1}/{max_retries}")
                        if attempt == max_retries - 1:
                            return text
                        time.sleep(1)
                else:
                    print(f"âœ— Grok translation error: {response.status_code}")
                    if attempt == max_retries - 1:
                        return text
                    time.sleep(1)
                    
            except Exception as e:
                print(f"âœ— Translation error (attempt {attempt+1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    return text
                time.sleep(1)
        
        return text
    
    def translate_with_gemini(self, text, max_retries=2):
        """Gemini APIë¡œ ë²ˆì—­"""
        if not GEMINI_API_KEY:
            print("Warning: GEMINI_API_KEY not set, skipping translation")
            return text
        
        if not text or len(text.strip()) == 0:
            return text
            
        for attempt in range(max_retries):
            try:
                model = genai.GenerativeModel('gemini-2.5-flash')
                prompt = f"""Translate the following English text to natural Korean. 
Only output the Korean translation, nothing else:

{text}"""
                
                response = model.generate_content(prompt)
                translated = response.text.strip()
                
                # ë²ˆì—­ì´ ì œëŒ€ë¡œ ë˜ì—ˆëŠ”ì§€ í™•ì¸ (í•œê¸€ í¬í•¨ ì—¬ë¶€)
                if any('\uac00' <= c <= '\ud7a3' for c in translated):
                    print(f"âœ“ Translated: {text[:40]}... -> {translated[:40]}...")
                    return translated
                else:
                    print(f"âœ— Translation failed (no Korean detected), retry {attempt+1}/{max_retries}")
                    if attempt == max_retries - 1:
                        return text
                    time.sleep(1)
                    
            except Exception as e:
                print(f"âœ— Translation error (attempt {attempt+1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    import traceback
                    traceback.print_exc()
                    return text
                time.sleep(1)
        
        return text
    
    def fetch_rss_feed(self):
        """GlobeNewswire RSS í”¼ë“œì—ì„œ ìµœì‹  ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì²´í¬ + Raw ì €ì¥)"""
        try:
            rss_url = "https://www.globenewswire.com/RssFeed/subjectcode/11-technology/feedTitle/GlobeNewswire%20-%20Technology"
            feed = feedparser.parse(rss_url)
            
            new_news_list = []
            skipped_count = 0
            
            for entry in feed.entries[:30]:  # ìµœì‹  30ê°œ í™•ì¸
                try:
                    # ë‰´ìŠ¤ ID ìƒì„±
                    news_id = hashlib.md5(entry.link.encode()).hexdigest()
                    
                    # ì¤‘ë³µ ì²´í¬ (API í˜¸ì¶œ ì „!)
                    if news_id in self.seen_news:
                        skipped_count += 1
                        continue
                    
                    # ë‰´ìŠ¤ ë°ì´í„° ìƒì„± (ì˜ì–´ ì›ë³¸)
                    news_data = {
                        'id': news_id,
                        'region': 'Y',
                        'publishedTime': entry.get('published', ''),
                        'source': entry.get('author', 'Unknown'),
                        'title': entry.get('title', ''),
                        'link': entry.get('link', ''),
                        'imageUrl': '',
                        'description': entry.get('summary', '')[:300],
                        'content': '',  # ë‚˜ì¤‘ì— fetch
                    }
                    
                    # ì¦‰ì‹œ Raw ì €ì¥ (API í˜¸ì¶œ ì „!)
                    self.save_raw_news(news_data)
                    new_news_list.append(news_data)
                    
                except Exception as e:
                    print(f"âœ— RSS entry parse error: {e}")
                    continue
            
            if skipped_count > 0:
                print(f"â­ï¸  [RSS] {skipped_count}ê°œ ì¤‘ë³µ ê±´ë„ˆëœ€ (ì´ë¯¸ ì²˜ë¦¬ë¨)")
            
            return new_news_list
        except Exception as e:
            print(f"âœ— RSS feed fetch error: {e}")
            return []
    
    def save_to_file(self, news_data):
        """ë°±ì—”ë“œë¡œ ì „ì†¡ (íŒŒì¼ ì €ì¥ì€ ë°±ì—”ë“œì—ì„œ ì²˜ë¦¬)"""
        try:
            # ë°±ì—”ë“œë¡œ ì „ì†¡
            response = requests.post(f"{BACKEND_URL}/api/news", json=news_data, timeout=10)
            if response.status_code == 200:
                print(f"ğŸ“¡ ë°±ì—”ë“œ ì „ì†¡ ì„±ê³µ: {news_data['title'][:50]}...")
            else:
                print(f"âš ï¸  ë°±ì—”ë“œ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
        except Exception as e:
            print(f"âœ— ë°±ì—”ë“œ ì „ì†¡ ì‹¤íŒ¨: {e}")
    
    def monitor(self):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§"""
        print("Starting GlobeNewswire scraper...")
        self.setup_driver()
        
        try:
            # 40ê°œì”© ê°€ì ¸ì˜¤ëŠ” ë‰´ìŠ¤ë£¸ í˜ì´ì§€ë¡œ ë³€ê²½
            news_url = 'https://www.globenewswire.com/NewsRoom?page=1&pageSize=40'
            self.driver.get(news_url)
            time.sleep(5)
            
            print("Monitoring started. Fetching 40 articles per cycle...")
            print(f"Target URL: {news_url}")
            
            # ì´ˆê¸° 40ê°œ ë¡œë“œ
            print("\n" + "="*60)
            print("ì´ˆê¸° ë¡œë”©: 40ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
            print("="*60)
            
            while True:
                try:
                    # 1ë‹¨ê³„: RSS í”¼ë“œì—ì„œ ìµœì‹  ë‰´ìŠ¤ ë¨¼ì € í™•ì¸ (ë¹ ë¦„ + Raw ì €ì¥ ì™„ë£Œ)
                    print("\nğŸ” RSS í”¼ë“œ í™•ì¸ ì¤‘...")
                    rss_news = self.fetch_rss_feed()  # ì´ë¯¸ ì¤‘ë³µ ì²´í¬ + Raw ì €ì¥ ì™„ë£Œ
                    
                    if rss_news:
                        print(f"ğŸ’¾ RSSì—ì„œ {len(rss_news)}ê°œ ì‹ ê·œ ë‰´ìŠ¤ â†’ Raw ì €ì¥ ì™„ë£Œ")
                        
                        # Content ê°€ì ¸ì˜¤ê¸°
                        for news_data in rss_news:
                            content = self.fetch_news_content(news_data['link'])
                            news_data['content'] = content
                            news_data['description'] = content[:300] if content else news_data['title']
                        
                        print(f"\n{'='*60}")
                        print(f"ğŸš€ RSS: {len(rss_news)}ê°œ AI ë¶„ì„ ì‹œì‘")
                        print(f"{'='*60}")
                        
                        analyzed_results = self.analyze_batch_with_gemini(rss_news)
                        
                        for result in analyzed_results:
                            news_data = result['news']
                            analysis = result['analysis']
                            
                            if not analysis.get('isNasdaqListed'):
                                continue
                            
                            news_data['analysis'] = analysis
                            news_data['titleKo'] = analysis.get('titleKo', news_data['title'])
                            news_data['descriptionKo'] = analysis.get('descriptionKo', news_data['description'])
                            
                            try:
                                import pytz
                                time_str = news_data['publishedTime'].replace(' ET', '')
                                dt = datetime.strptime(time_str, "%B %d, %Y %H:%M")
                                et_tz = pytz.timezone('US/Eastern')
                                dt_et = et_tz.localize(dt)
                                kr_tz = pytz.timezone('Asia/Seoul')
                                dt_kr = dt_et.astimezone(kr_tz)
                                news_data['publishedTimeKo'] = dt_kr.strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')
                            except:
                                news_data['publishedTimeKo'] = news_data['publishedTime']
                            
                            self.save_to_file(news_data)
                            print(f"âœ… [RSS] {analysis.get('ticker')} - {news_data['titleKo'][:40]}...")
                    else:
                        print(f"â­ï¸  RSS: ì‹ ê·œ ë‰´ìŠ¤ ì—†ìŒ")
                    
                    # 2ë‹¨ê³„: Featured Releases ì„¹ì…˜ì˜ ëª¨ë“  ë‰´ìŠ¤ í•­ëª© ì°¾ê¸° (ê¸°ì¡´ ë°©ì‹)
                    news_items = []
                    
                    # ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„
                    selectors = [
                        'div.featured-releases article',
                        'div.featured-releases > *',
                        'article',
                        'div[class*="article"]',
                        'li'
                    ]
                    
                    for selector in selectors:
                        try:
                            items = self.driver.find_elements(By.CSS_SELECTOR, selector)
                            if len(items) >= 10:  # ìµœì†Œ 10ê°œ ì´ìƒ ì°¾ìœ¼ë©´ ì„±ê³µ
                                news_items = items
                                print(f"âœ“ Found {len(items)} items with selector: {selector}")
                                break
                        except:
                            continue
                    
                    if not news_items:
                        print("âš ï¸  No news items found. Retrying...")
                        time.sleep(2)
                        self.driver.refresh()
                        time.sleep(3)
                        continue
                    
                    # ìµœëŒ€ 40ê°œê¹Œì§€ ì²˜ë¦¬ (ë°°ì¹˜ ë°©ì‹)
                    processed = 0
                    skipped = 0
                    
                    # 1ë‹¨ê³„: ëª¨ë“  ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ (ì¤‘ë³µ ì²´í¬ + Raw ì €ì¥)
                    batch_news = []
                    for item in news_items[:40]:
                        try:
                            # ë§í¬ê°€ ìˆëŠ”ì§€ í™•ì¸
                            try:
                                link_elem = item.find_element(By.CSS_SELECTOR, 'a')
                            except:
                                skipped += 1
                                continue
                            
                            news_data = self.parse_news_item(item)
                            if not news_data:
                                skipped += 1
                                continue
                            
                            # ì¤‘ë³µ ì²´í¬ (Raw íŒŒì¼ ê¸°ë°˜!)
                            if news_data['id'] in self.seen_news:
                                skipped += 1
                                continue
                            
                            print(f"\n{'='*60}")
                            print(f"New article found: {news_data['title'][:50]}...")
                            print(f"Link: {news_data['link']}")
                            
                            # ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
                            content = self.fetch_news_content(news_data['link'])
                            news_data['description'] = content[:300] if content else news_data['title']
                            news_data['content'] = content  # ì „ì²´ ë‚´ìš© ì €ì¥
                            
                            # ì¦‰ì‹œ Raw ì €ì¥ (API í˜¸ì¶œ ì „!)
                            self.save_raw_news(news_data)
                            batch_news.append(news_data)
                            
                        except Exception as e:
                            print(f"âœ— Error collecting news: {e}")
                            skipped += 1
                    
                    # 2ë‹¨ê³„: ë°°ì¹˜ ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬, Raw ì €ì¥ ì™„ë£Œ ìƒíƒœ)
                    if batch_news:
                        print(f"\n{'='*60}")
                        print(f"ğŸš€ HTML: {len(batch_news)}ê°œ AI ë¶„ì„ ì‹œì‘ (Raw ì €ì¥ ì™„ë£Œ)")
                        print(f"{'='*60}")
                        
                        analyzed_results = self.analyze_batch_with_gemini(batch_news)
                        
                        # 3ë‹¨ê³„: ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬
                        for result in analyzed_results:
                            news_data = result['news']
                            analysis = result['analysis']
                            
                            # ë‚˜ìŠ¤ë‹¥ ìƒì¥ ì¢…ëª©ì´ ì•„ë‹ˆë©´ ê±´ë„ˆë›°ê¸° (seen_newsëŠ” ì´ë¯¸ Rawì— ì¶”ê°€ë¨)
                            if not analysis.get('isNasdaqListed'):
                                print(f"â­ï¸  Skipped: {news_data['title'][:30]}... (Not NASDAQ)")
                                skipped += 1
                                continue
                            
                            # ë‚˜ìŠ¤ë‹¥ ìƒì¥ ì¢…ëª©ë§Œ ì²˜ë¦¬
                            news_data['analysis'] = analysis
                            ticker_info = f"[{analysis.get('ticker')}]" if analysis.get('ticker') else "[N/A]"
                            print(f"\nâœ… NASDAQ: {ticker_info} {news_data['title'][:40]}...")
                            print(f"   Sentiment: {analysis.get('sentiment', 'N/A')} - {analysis.get('positivePercentage', 0)}% í˜¸ì¬")
                            
                            # AIê°€ ì´ë¯¸ ë²ˆì—­ì„ í¬í•¨í•˜ì—¬ ë°˜í™˜
                            news_data['titleKo'] = analysis.get('titleKo', news_data['title'])
                            news_data['descriptionKo'] = analysis.get('descriptionKo', news_data['description'])
                            
                            print(f"   Korean: {news_data['titleKo'][:40]}...")
                            
                            # ê²Œì œ ì‹œê°„ í•œêµ­ ë³€í™˜ ì¶”ê°€
                            try:
                                import pytz
                                
                                # ì›ë³¸ ì‹œê°„ì„ íŒŒì‹± (ET ì‹œê°„ëŒ€)
                                time_str = news_data['publishedTime'].replace(' ET', '')
                                dt = datetime.strptime(time_str, "%B %d, %Y %H:%M")
                                
                                # ET ì‹œê°„ëŒ€ ì„¤ì •
                                et_tz = pytz.timezone('US/Eastern')
                                dt_et = et_tz.localize(dt)
                                
                                # í•œêµ­ ì‹œê°„ëŒ€ë¡œ ë³€í™˜
                                kr_tz = pytz.timezone('Asia/Seoul')
                                dt_kr = dt_et.astimezone(kr_tz)
                                
                                # í•œêµ­ ì‹œê°„ ë¬¸ìì—´ ìƒì„±
                                news_data['publishedTimeKo'] = dt_kr.strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')
                            except Exception as e:
                                print(f"âš ï¸  Time conversion error: {e}")
                                news_data['publishedTimeKo'] = news_data['publishedTime']
                            
                            # íŒŒì¼ ë° ë°±ì—”ë“œë¡œ ì €ì¥ (ë‚˜ìŠ¤ë‹¥ ì¢…ëª©ë§Œ)
                            self.save_to_file(news_data)
                            self.seen_news.add(news_data['id'])
                            processed += 1
                            print(f"   ğŸ’¾ Saved: #{processed}")
                    
                    # ê±´ë„ˆë›´ ë‰´ìŠ¤ ì²˜ë¦¬
                    for news_data in batch_news:
                        if news_data['id'] not in self.seen_news:
                            self.seen_news.add(news_data['id'])
                    
                    # ì²˜ë¦¬ ê²°ê³¼ ì¶œë ¥
                    print(f"\nğŸ“Š ì²˜ë¦¬ ê²°ê³¼: ì´ {processed}ê°œ ì²˜ë¦¬, {skipped}ê°œ ìŠ¤í‚µ")
                    print(f"ğŸ’¾ Raw DB: {len(self.seen_news)}ê°œ ë‰´ìŠ¤ (ì¤‘ë³µ ë°©ì§€)")
                    
                    # 1ì´ˆ ëŒ€ê¸° í›„ ìƒˆë¡œê³ ì¹¨ (RSSë¡œ ì¸í•´ ì†ë„ í–¥ìƒ)
                    print(f"\nâ±ï¸  1ì´ˆ í›„ ìƒˆë¡œê³ ì¹¨...")
                    time.sleep(1)
                    self.driver.refresh()
                    time.sleep(0.5)
                    
                except Exception as e:
                    print(f"âœ— Error in monitoring loop: {e}")
                    import traceback
                    traceback.print_exc()
                    time.sleep(5)
                    try:
                        print("ğŸ”„ Refreshing page...")
                        self.driver.refresh()
                        time.sleep(3)
                    except:
                        print("âœ— Driver error, restarting...")
                        self.driver.quit()
                        self.setup_driver()
                        self.driver.get(news_url)
                        time.sleep(5)
                    
        except KeyboardInterrupt:
            print("\nStopping scraper...")
        finally:
            if self.driver:
                self.driver.quit()

if __name__ == '__main__':
    scraper = GlobeNewswireScraper()
    scraper.monitor()

