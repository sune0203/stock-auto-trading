# offline/train_daily.py
"""
ì „ì—­ ëª¨ë¸ + í‹°ì»¤ë³„ í†µê³„ í•™ìŠµ
XGBoostë¥¼ ì‚¬ìš©í•œ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ (Windows í˜¸í™˜)
"""
import os
import json
import pickle
import sys
from pathlib import Path

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, classification_report
from xgboost import XGBClassifier

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ ê²½ë¡œì— ì¶”ê°€
ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT))

DATA_DIR = ROOT / "data"
FEATURES_PATH = DATA_DIR / "offline_features.parquet"
MODEL_PATH = DATA_DIR / "model_xgb_30m.json"  # XGBoostëŠ” json í˜•ì‹
SYM_STATS_PATH = DATA_DIR / "symbol_stats.json"

# í”¼ì²˜ ì»¬ëŸ¼ (15ë¶„ë´‰ ê¸°ì¤€)
FEATURES = ["rvol_15m", "base_range", "spread_est", "move_prev"]
TARGET = "label_30m"  # 15ë¶„ë´‰ 2ê°œ = 30ë¶„

def compute_sym_stats(df: pd.DataFrame) -> dict:
    """
    í‹°ì»¤ë³„ ì„±ê³µ ì¼€ì´ìŠ¤ í†µê³„ ê³„ì‚°
    """
    out = {}
    
    for sym, g in df.groupby("symbol"):
        # ì„±ê³µí•œ ì¼€ì´ìŠ¤ë§Œ í•„í„°ë§
        ok = g[g[TARGET] == 1]
        
        if len(ok) == 0:
            continue
        
        # ì„±ê³µ ì¼€ì´ìŠ¤ì˜ ì£¼ìš” ì§€í‘œ ë¶„ìœ„ìˆ˜
        out[sym] = {
            "total_events": len(g),
            "success_events": len(ok),
            "success_rate": float(len(ok) / len(g)),
            "rvol_success_q60": float(ok["rvol_15m"].quantile(0.60)),
            "spread_success_q90": float(ok["spread_est"].quantile(0.90)),
            "score_success_q70": 0.65,  # ì´ˆê¸°ê°’, ë‚˜ì¤‘ì— ì ìˆ˜ ë¶„í¬ë¡œ ì¡°ì •
        }
    
    return out

def main():
    """
    ëª¨ë¸ í•™ìŠµ ë° í†µê³„ ì €ì¥
    """
    print("=" * 70)
    print("ğŸ¤– ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("=" * 70)
    
    # 1. ë°ì´í„° ë¡œë“œ
    if not FEATURES_PATH.exists():
        print(f"âŒ ì˜¤í”„ë¼ì¸ í”¼ì²˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {FEATURES_PATH}")
        print("ğŸ’¡ features_offline.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    print(f"   ê²½ë¡œ: {FEATURES_PATH}")
    df = pd.read_parquet(FEATURES_PATH)
    print(f"   âœ… ì›ë³¸ ë°ì´í„°: {len(df):,}í–‰")
    
    print(f"\nğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    df = df.dropna(subset=[TARGET] + FEATURES).copy()
    print(f"   âœ… ê²°ì¸¡ì¹˜ ì œê±° í›„: {len(df):,}í–‰")
    
    print(f"\nğŸ“Š ë°ì´í„° ë¶„í¬:")
    print(f"   ì¢…ëª© ìˆ˜: {df['symbol'].nunique()}ê°œ")
    print(f"   ê¸°ê°„: {df['ts'].min()} ~ {df['ts'].max()}")
    
    print(f"\nğŸ“ˆ íƒ€ê²Ÿ ë¶„í¬ ({TARGET}):")
    target_counts = df[TARGET].value_counts().sort_index()
    for label, count in target_counts.items():
        label_name = "ì„±ê³µ" if label == 1 else "ì‹¤íŒ¨"
        print(f"   {label_name}(={label}): {count:4d} ({count/len(df)*100:5.1f}%)")

    # 2. ë°ì´í„° ì¶©ë¶„ì„± ì²´í¬
    if len(df) < 200:
        print(f"\nâŒ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (í˜„ì¬: {len(df)}ê°œ, ìµœì†Œ: 200ê°œ)")
        print("ğŸ’¡ ë” ë§ì€ ì¢…ëª©ì„ ìŠ¤ìº”í•˜ê±°ë‚˜ ì¡°ê±´ì„ ì™„í™”í•˜ì„¸ìš”.")
        return

    # 3. í”¼ì²˜/íƒ€ê²Ÿ ë¶„ë¦¬
    print(f"\nğŸ“Š í”¼ì²˜/íƒ€ê²Ÿ ë¶„ë¦¬ ì¤‘...")
    print(f"   í”¼ì²˜: {FEATURES}")
    print(f"   íƒ€ê²Ÿ: {TARGET}")
    X = df[FEATURES].values
    y = df[TARGET].values.astype(int)
    print(f"   âœ… X shape: {X.shape}, y shape: {y.shape}")

    # 4. Train/Test ë¶„í• 
    print(f"\nâœ‚ï¸ Train/Test ë¶„í•  ì¤‘... (Test=20%)")
    X_tr, X_te, y_tr, y_te = train_test_split(
        X, y, test_size=0.2, shuffle=True, random_state=42
    )
    
    print(f"   Train: {len(X_tr):,}ê°œ ({len(X_tr)/len(X)*100:.1f}%)")
    print(f"   Test:  {len(X_te):,}ê°œ ({len(X_te)/len(X)*100:.1f}%)")
    
    # Train íƒ€ê²Ÿ ë¶„í¬
    train_success = (y_tr == 1).sum()
    print(f"   Train íƒ€ê²Ÿ: ì„±ê³µ={train_success} ({train_success/len(y_tr)*100:.1f}%), ì‹¤íŒ¨={len(y_tr)-train_success}")

    # 5. ëª¨ë¸ í•™ìŠµ
    print(f"\nğŸ¤– ëª¨ë¸ í•™ìŠµ ì¤‘...")
    print(f"   ì•Œê³ ë¦¬ì¦˜: XGBoost Classifier (Windows ìµœì í™”)")
    print(f"   ì„¤ì •:")
    print(f"     - n_estimators: 400")
    print(f"     - learning_rate: 0.03")
    print(f"     - max_depth: 6")
    print(f"     - scale_pos_weight: auto")
    
    # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬
    scale_pos_weight = (y_tr == 0).sum() / (y_tr == 1).sum()
    
    model = XGBClassifier(
        n_estimators=400,
        learning_rate=0.03,
        max_depth=6,
        scale_pos_weight=scale_pos_weight,
        random_state=42,
        verbosity=0,  # ë¡œê·¸ ìµœì†Œí™”
        use_label_encoder=False,
        eval_metric='logloss'
    )
    model.fit(X_tr, y_tr)
    print(f"   âœ… í•™ìŠµ ì™„ë£Œ!")

    # 6. ê²€ì¦
    print("\n" + "=" * 70)
    print("ğŸ“Š ê²€ì¦ ê²°ê³¼")
    print("=" * 70)
    
    print(f"\nğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
    prob_te = model.predict_proba(X_te)[:, 1]
    y_pred = model.predict(X_te)
    print(f"   âœ… ì˜ˆì¸¡ ì™„ë£Œ")
    
    # AUC ê³„ì‚°
    try:
        auc = roc_auc_score(y_te, prob_te)
        print(f"\nğŸ“ˆ AUC Score: {auc:.4f}")
        if auc > 0.7:
            print(f"   ğŸ¯ ìš°ìˆ˜í•œ ì„±ëŠ¥!")
        elif auc > 0.6:
            print(f"   âœ… ì–‘í˜¸í•œ ì„±ëŠ¥")
        else:
            print(f"   âš ï¸ ì„±ëŠ¥ ê°œì„  í•„ìš”")
    except Exception as e:
        print(f"\nâš ï¸ AUC ê³„ì‚° ë¶ˆê°€: {e}")

    # ë¶„ë¥˜ ë¦¬í¬íŠ¸
    print(f"\nğŸ“Š ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
    print("=" * 70)
    print(classification_report(y_te, y_pred, target_names=["ì‹¤íŒ¨", "ì„±ê³µ"]))
    print("=" * 70)

    # 7. ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ì ìˆ˜ ê³„ì‚°
    print(f"\nğŸ”® ì „ì²´ ë°ì´í„° ì ìˆ˜ ê³„ì‚° ì¤‘...")
    df["score"] = model.predict_proba(df[FEATURES].values)[:, 1]
    print(f"   âœ… ì™„ë£Œ")
    print(f"   ì ìˆ˜ ë²”ìœ„: {df['score'].min():.4f} ~ {df['score'].max():.4f}")
    print(f"   ì ìˆ˜ í‰ê· : {df['score'].mean():.4f}")
    
    # 8. í‹°ì»¤ë³„ í†µê³„ ê³„ì‚°
    print(f"\nğŸ“Š í‹°ì»¤ë³„ í†µê³„ ê³„ì‚° ì¤‘...")
    sym_stats = compute_sym_stats(df)
    print(f"   âœ… {len(sym_stats)}ê°œ ì¢…ëª© í†µê³„ ìƒì„± ì™„ë£Œ")
    
    if len(sym_stats) > 0:
        # ìƒìœ„ 5ê°œ ì¢…ëª© í†µê³„ ì¶œë ¥
        sorted_stats = sorted(sym_stats.items(), key=lambda x: x[1]['success_rate'], reverse=True)
        print(f"\n   ğŸ† ì„±ê³µë¥  ìƒìœ„ 5ê°œ ì¢…ëª©:")
        for i, (sym, stat) in enumerate(sorted_stats[:5], 1):
            print(f"      {i}. {sym:6s}: ì„±ê³µë¥  {stat['success_rate']*100:.1f}% "
                  f"({stat['success_events']}/{stat['total_events']} ì´ë²¤íŠ¸)")

    # 9. ì €ì¥
    print("\n" + "=" * 70)
    print("ğŸ’¾ íŒŒì¼ ì €ì¥")
    print("=" * 70)
    
    DATA_DIR.mkdir(exist_ok=True)
    
    # ëª¨ë¸ ì €ì¥ (XGBoostëŠ” JSON í˜•ì‹)
    print(f"\nğŸ“¦ ëª¨ë¸ ì €ì¥ ì¤‘...")
    print(f"   ê²½ë¡œ: {MODEL_PATH}")
    model.save_model(str(MODEL_PATH))
    print(f"   âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ ({MODEL_PATH.stat().st_size / 1024:.1f} KB)")
    
    # í†µê³„ ì €ì¥
    print(f"\nğŸ“Š í†µê³„ ì €ì¥ ì¤‘...")
    print(f"   ê²½ë¡œ: {SYM_STATS_PATH}")
    with open(SYM_STATS_PATH, "w") as f:
        json.dump(sym_stats, f, indent=2)
    print(f"   âœ… í†µê³„ ì €ì¥ ì™„ë£Œ ({SYM_STATS_PATH.stat().st_size / 1024:.1f} KB)")
    
    # 10. í”¼ì²˜ ì¤‘ìš”ë„
    print("\n" + "=" * 70)
    print("ğŸ” í”¼ì²˜ ì¤‘ìš”ë„")
    print("=" * 70)
    
    importance = pd.DataFrame({
        'feature': FEATURES,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print()
    for idx, row in importance.iterrows():
        feat = row['feature']
        imp = row['importance']
        bar_len = int(imp * 50)  # 0~1 ë²”ìœ„ë¥¼ 0~50ìœ¼ë¡œ ë³€í™˜
        bar = 'â–ˆ' * bar_len
        print(f"   {feat:15s} | {bar} {imp:.4f}")
    
    # ìµœì¢… ë¦¬í¬íŠ¸
    print("\n" + "=" * 70)
    print("ğŸ¯ í•™ìŠµ ì™„ë£Œ!")
    print("=" * 70)
    print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
    print(f"   1. {MODEL_PATH}")
    print(f"   2. {SYM_STATS_PATH}")
    
    print(f"\nğŸ“Š ìš”ì•½:")
    print(f"   í•™ìŠµ ë°ì´í„°: {len(X_tr):,}ê°œ")
    print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_te):,}ê°œ")
    print(f"   ì¢…ëª© í†µê³„: {len(sym_stats)}ê°œ")
    
    print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   python server/server.py")
    print("=" * 70)

if __name__ == "__main__":
    main()

